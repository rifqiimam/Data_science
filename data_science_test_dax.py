# -*- coding: utf-8 -*-
"""data science test DAX

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E7FY4i58xZzLYVrK4qDWbhO5fIjDo6x6

#EDA

##Exploratory Data
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

# %config InlineBackend.figure_format = 'svg'

from scipy import stats
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error

from google.colab import files
uploaded = files.upload()

df = pd.read_csv('/content/Data.csv')

df.head(5)

df.info()

df.shape

df.isnull().sum()

df.describe().T

"""# Data Visualization

##Data Visualization
"""

plt.scatter(df['Math'],df['Statistics'],marker = '*', color = 'g')
plt.scatter(df['Math'],df['Science'],marker = '+', color = 'b')
plt.scatter(df['Math'],df['Pysics'],marker = '.', color = 'y')

plt.figure(figsize=(16, 6))
mask = np.triu(np.ones_like(df.corr(), dtype=np.bool))
#heatmap = sns.heatmap(df.corr()[['Math']].sort_values(by='Math', ascending=False), vmin=-1, vmax=1, cmap="crest", annot=True, linewidths=.5)
heatmap = sns.heatmap(df.corr(), mask=mask, vmin=-1, vmax=1, annot=True, cmap="crest", linewidths=.5)
heatmap.set_title(' Correlation with each independent variable to target variable Math Heatmap')

heatmap = sns.heatmap(df.corr()[['Math']].sort_values(by='Math', ascending=False), vmin=-1, vmax=1, cmap="crest", annot=True, linewidths=.5)

df.plot(kind='density',subplots=True, layout=(10,5), 
                  sharex=False, figsize=(20,12))
plt.ylabel(' ')
plt.show()

df.hist(figsize=(15, 8), bins=10)

plt.show()

fig = plt.figure(figsize=[32,24])
fig.subplots_adjust(top=0.92, hspace=0.5, wspace=0.4)
for i, col in enumerate(df):
  ax = fig.add_subplot(25, 5, i+1)
  sns.boxplot(data=df, x=col, ax=ax)
  ax.set_title(f'{col}')
  ax.set_xlabel(f'{col}')

"""## Data Preprocessing

Outliers Handling
"""

dict={}
for col in df:
  percentile25 = df[col].quantile(0.25)
  percentile75 = df[col].quantile(0.75)
  IQR = percentile75-percentile25
  upper_limit = percentile75+1.5*IQR
  lower_limit = percentile25-1.5*IQR
  dict['upper_limit'+'_'+col] = upper_limit
  dict['lower_limit'+'_'+col] = lower_limit

for col in df:
  df[col]= np.where(
      df[col]>dict['upper_limit_'+col],dict['upper_limit_'+col],
      np.where(
          df[col]<dict['lower_limit_'+col],dict['lower_limit_'+col],df[col]
      )
  )

fig = plt.figure(figsize=[32,24])
fig.subplots_adjust(top=0.92, hspace=0.5, wspace=0.4)
for i, col in enumerate(df):
  ax = fig.add_subplot(25, 5, i+1)
  sns.boxplot(data=df, x=col, ax=ax)
  ax.set_title(f'{col}')
  ax.set_xlabel(f'{col}')

"""Split Feature and Targeted Feature"""

X = df.iloc[:,:-1].values
y = df.iloc[:,-1].values
st = df.iloc[:,3].values
py = df.iloc[:,1].values
sc = df.iloc[:,2].values

X.shape

y.shape

"""Split Data Set"""

X_train, X_test , y_train, y_test = train_test_split(X,y, test_size=0.2, random_state = 42)

X_train, X_test , y_train, y_test = train_test_split(st,y, test_size=0.2, random_state = 42)

X_train1 = np.reshape(X_train, (-1, 1))
X_test1 = np.reshape(X_test, (-1, 1))

"""Scalling data

NOTED : there is some interesting happen when using robust scalling with out outlier handling we got better score but with outlier handling we got worse score this need to be explore more
"""

from sklearn.preprocessing import RobustScaler
rb= RobustScaler()
X_train1 = rb.fit_transform(X_train1)
X_test1 = rb.fit_transform(X_test1)

"""NOTED : normal interaction whe using outlier handling using STD and minmax scalling that it got better with oulier handling"""

from sklearn.preprocessing import StandardScaler
sc1 = StandardScaler()
X_train1 = sc1.fit_transform(X_train1)
X_test1 = sc1.fit_transform(X_test1)

from sklearn.preprocessing import MinMaxScaler
sc = MinMaxScaler()
X = sc.fit_transform(X)

"""Principal Component Analysis"""

from sklearn.decomposition import PCA
pca = PCA(n_components=1)
fit_pca = pca.fit_transform(X_train)
pca_df = pd.DataFrame(data = fit_pca, columns = ['PCA_1'])

pca = PCA(n_components=1)
fit_pca = pca.fit_transform(X_test)
X_testpca = pd.DataFrame(data = fit_pca, columns = ['PCA_1'])

"""#Machine Learning Model

if the score of model machine learning give negative value thats mean the model is not recomended to used for regression model because it means the model is very poorly and really need to make an improve value in R^2/R-Squared score this accur when i used 3 variable response to make ML model

so in this jupyter notebook i run the model used 1 variable response Statistics

##Logistic Regresion
"""

from sklearn.linear_model import LogisticRegression
LR = LogisticRegression()
LR.fit(X_train1, y_train)
resultLR = LR.score(X_test1, y_test)

print(f'Test score: {resultLR:.2f}')

"""MSE"""

y_pred = LR.predict(X_test1)
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)

"""MAE"""

from sklearn.metrics import mean_absolute_error
y_pred = LR.predict(X_test1)
mae = mean_absolute_error(y_test, y_pred)
print("MAE scores:\n", mae)

"""##Decision Tree Regression"""

from sklearn.tree import DecisionTreeRegressor
dtr = DecisionTreeRegressor(random_state=0)
dtr.fit(X_train1, y_train)

"""Accuracy"""

score = dtr.score(X_test1, y_test)
print(f'Test score: {score:.2f}')

"""MSE"""

y_pred = dtr.predict(X_test1)
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)

"""MAE"""

from sklearn.metrics import mean_absolute_error
y_pred = dtr.predict(X_test1)
mae = mean_absolute_error(y_test, y_pred)
print("MAE scores:\n", mae)

from sklearn.metrics import mean_absolute_percentage_error
y_pred = dtr.predict(X_test1)
mape = mean_absolute_error(y_test, y_pred)
print("MAPE scores:\n", mape)

"""## Random Forest

"""

from sklearn.ensemble import RandomForestRegressor
rfr = RandomForestRegressor(n_estimators = 100, random_state = 0)
rfr.fit(X_train1, y_train)

"""Accuracy"""

score = rfr.score(X_test1, y_test)
print(f'Test score: {score:.2f}')

"""MSE"""

y_pred = rfr.predict(X_test1)
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)

"""MAE"""

from sklearn.metrics import mean_absolute_error
y_pred = rfr.predict(X_test1)
mae = mean_absolute_error(y_test, y_pred)
print("MAE scores:\n", mae)

from sklearn.metrics import mean_absolute_percentage_error
y_pred = rfr.predict(X_test1)
mape = mean_absolute_error(y_test, y_pred)
print("MAPE scores:\n", mape)

"""##Multiple Linier Regresion"""

from sklearn import linear_model

linreg = linear_model.LinearRegression()
linreg.fit(X_train1, y_train)

"""Accuracy"""

score = linreg.score(X_test1, y_test)
print(f'Test score: {score:.2f}')

"""MSE"""

y_pred = linreg.predict(X_test1)
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)

"""MAE"""

y_pred = linreg.predict(X_test1)
mae = mean_absolute_error(y_test, y_pred)
print("MAE scores:\n", mae)

"""##Gradient Boosting Regressor"""

from sklearn.ensemble import GradientBoostingRegressor
for i in [0.1, 0.01, 0.001]:
    for j in [100, 250, 500, 750]:
        gbt = GradientBoostingRegressor(learning_rate=i, n_estimators=j)
        gbt = gbt.fit(X_train1, y_train)
        print("predict output for GradientBoostingRegressor: learning_rate={}, n_estimators={}".format(i, j))
        mse = mean_squared_error(y_test, gbt.predict(X_test1))
        print("The mean squared error (MSE) on test set: {:.4f}".format(mse))

        pred2 = gbt.predict(X_test1)
        print("Accuracy on training set: %.3f" % gbt.score(X_train1, y_train))
        print("Accuracy on test set: %.3f" % gbt.score(X_test1, y_test))
        print("==============================================")

from sklearn.ensemble import GradientBoostingRegressor
gbt = GradientBoostingRegressor(learning_rate=0.1, n_estimators=500, random_state=0)

gbt = gbt.fit(X_train1, y_train)
print("predict output for GradientBoostingRegressor: learning_rate={}, n_estimators={}".format(i, j))
mse = mean_squared_error(y_test, gbt.predict(X_test1))
print("The mean squared error (MSE) on test set: {:.4f}".format(mse))

pred2 = gbt.predict(X_test1)
print("The best accuracy on training set: %.3f" % gbt.score(X_train1, y_train))
print("The best accuracy on test set: %.3f" % gbt.score(X_test1, y_test))

score = gbt.score(X_test1, y_test)
print(f'Test score: {score:.2f}')

"""MAE"""

y_pred = gbt.predict(X_test1)
mae = mean_absolute_error(y_test, y_pred)
print("MAE scores:\n", mae)

"""CONCLUSION COTINUOUS DATA NOT RECOMEND TO USE GRADIENT BOOSTING REGRESION BECAUSE SCORE THA IT GIVEN IN R SQUARED/R^2 IS NEGATIVE VALUE WHICH IS IMPOSIBLE TO BE HAPPEN

## Support Vektor Regressor

SVR
"""

from sklearn.svm import SVR
# create the model object
svr = SVR(kernel = 'rbf')
# fit the model on the data
svr.fit(X_train1, y_train)

"""Accuracy"""

score = svr.score(X_test1, y_test)
print(f'Test score: {score:.2f}')

"""MAE"""

y_pred = svr.predict(X_test1)
mae = mean_absolute_error(y_test, y_pred)
print("MAE scores:\n", mae)

"""MSE"""

y_pred = svr.predict(X_test1)
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)

"""##KNN"""

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
knn.fit(X_train1, y_train)

score = knn.score(X_test1, y_test)
print(f'Test score: {score:.2f}')

"""MAE"""

y_pred = svr.predict(X_test1)
mae = mean_absolute_error(y_test, y_pred)
print("MAE scores:\n", mae)

"""MSE"""

y_pred = rfr.predict(X_test1)
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)